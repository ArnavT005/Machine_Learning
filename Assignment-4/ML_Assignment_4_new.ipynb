{"cells":[{"cell_type":"markdown","metadata":{"id":"crIL2Um2oIro"},"source":["**Ideas -**\n","1. Find/Use OCR for non-comp part<br>\n","2. Rotate all images thrice to create a larger training dataset.<br>\n","3. Convert image to 0-1 grayscale.<br>\n","4. Use attention based RNN for caption generation.<br>"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7891,"status":"ok","timestamp":1636608374388,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"vnWhNJC8M5d5"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","\n","from skimage import io, transform\n","\n","import matplotlib.pyplot as plt # for plotting\n","import numpy as np\n","import os\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4922,"status":"ok","timestamp":1636608434291,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"EILA9D5cCXMe","outputId":"7695aed4-1b52-4c5d-fa12-9ce14e45824f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":622,"status":"ok","timestamp":1636608437773,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"Zd8M5oIXGf3D","outputId":"e3282115-3e0d-46d2-8468-5b29e5c1c48e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/MyDrive/'\n","/\n"]}],"source":["%cd drive/MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIe01QGXedHL"},"outputs":[],"source":["!unzip train_data.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1636608419246,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"p8rO5dobw8Tm","outputId":"4e7fd638-e295-4434-feb7-b968090713e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: 'train_data'\n","/content\n","2\n","/\n"]}],"source":["%cd train_data\n","!ls | wc -l\n","%cd .."]},{"cell_type":"markdown","metadata":{"id":"kC9JUlQMM5d9"},"source":["### Image Transforms"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":538,"status":"ok","timestamp":1636607678233,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"g6ClqmTUM5d9"},"outputs":[],"source":["class Rescale(object):\n","    \"\"\"Rescale the image in a sample to a given size.\n","\n","    Args:\n","        output_size (tuple or int): Desired output size. If tuple, output is\n","            matched to output_size. If int, smaller of image edges is matched\n","            to output_size keeping aspect ratio the same.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, image):\n","        h, w = image.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","\n","        new_h, new_w = int(new_h), int(new_w)\n","        img = transform.resize(image, (new_h, new_w))\n","        return img\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __call__(self, image):\n","        # swap color axis because\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.transpose((2, 0, 1))\n","        return torch.tensor(image)\n","\n","\n","# IMAGE_RESIZE = (256, 256)\n","IMAGE_RESIZE = (224, 224)\n","# Sequentially compose the transforms\n","img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])\n"]},{"cell_type":"markdown","metadata":{"id":"K5CM4cBhM5eA"},"source":["### Captions Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2479,"status":"ok","timestamp":1636607686557,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"rbEQS053M5eA"},"outputs":[],"source":["class CaptionsPreprocessing:\n","    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n","\n","    Args:\n","        captions_file_path (string): captions tsv file path\n","    \"\"\"\n","    def __init__(self, captions_file_path):\n","        self.captions_file_path = captions_file_path\n","\n","        # max caption length\n","        self.maxLen = 0\n","\n","        # Read raw captions\n","        self.raw_captions_dict = self.read_raw_captions()\n","\n","        # Preprocess captions\n","        self.captions_dict = self.process_captions()\n","\n","        # Create vocabulary\n","        self.vocab = self.generate_vocabulary()\n","\n","\n","    def read_raw_captions(self):\n","        \"\"\"\n","        Returns:\n","            Dictionary with raw captions list keyed by image ids (integers)\n","        \"\"\"\n","\n","        captions_dict = {}\n","        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n","            for img_caption_line in f.readlines():\n","                img_captions = img_caption_line.strip().split('\\t')\n","                captions_dict[img_captions[0]] = img_captions[1]\n","                self.maxLen = max(self.maxLen, len(img_captions[1].split()) + 2)\n","\n","        return captions_dict\n","\n","    def process_captions(self):\n","        \"\"\"\n","        Use this function to generate dictionary and other preprocessing on captions\n","        \"\"\"\n","\n","        raw_captions_dict = self.raw_captions_dict\n","\n","        # Do the preprocessing here\n","        captions_dict = {}\n","        # add START and END token\n","        for k, v in raw_captions_dict.items():\n","            captions_dict[k] = '[START] ' + v + ' [END]'\n","\n","        return captions_dict\n","\n","    def generate_vocabulary(self):\n","        \"\"\"\n","        Use this function to generate dictionary and other preprocessing on captions\n","        \"\"\"\n","\n","        captions_dict = self.captions_dict\n","\n","        # Generate the vocabulary\n","        vocab = {'[PAD]': 0}\n","        idx = 1\n","        for caption in captions_dict.values():\n","            for word in caption.split():\n","                if word in ['[START]', '[END]']:\n","                    continue\n","                if word not in vocab:\n","                    vocab[word] = idx\n","                    idx += 1\n","        vocab['[START]'] = idx\n","        vocab['[END]'] = idx + 1\n","\n","        return vocab\n","\n","    def captions_transform(self, img_caption):\n","        \"\"\"\n","        Use this function to generate tensor tokens for the text captions\n","        Args:\n","            img_caption: caption for a particular image\n","        \"\"\"\n","        vocab = self.vocab\n","\n","        # Generate tensors\n","        tokens = [vocab[word] for word in img_caption.split()]\n","        length = len(tokens)\n","        tokens.extend([0 for _ in range(len(tokens), self.maxLen)])\n","        return torch.tensor(tokens), length\n","\n","# Set the captions tsv file path\n","CAPTIONS_FILE_PATH = 'Train_text.tsv'\n","captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"lzmf-nlhM5eC"},"source":["### Dataset Class"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":519,"status":"ok","timestamp":1636607691765,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"bUuWkFPpM5eD"},"outputs":[],"source":["class ImageCaptionsDataset(Dataset):\n","\n","    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n","        \"\"\"\n","        Args:\n","            img_dir (string): Directory with all the images.\n","            captions_dict: Dictionary with captions list keyed by image paths (strings)\n","            img_transform (callable, optional): Optional transform to be applied\n","                on the image sample.\n","\n","            captions_transform: (callable, optional): Optional transform to be applied\n","                on the caption sample (list).\n","        \"\"\"\n","        self.img_dir = img_dir\n","        self.captions_dict = captions_dict\n","        self.img_transform = img_transform\n","        self.captions_transform = captions_transform\n","\n","        self.image_ids = list(captions_dict.keys())\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_ids[idx]\n","        image = io.imread(img_name)\n","        image_90 = transform.rotate(image, 90)\n","        image_180 = transform.rotate(image, 180)\n","        image_270 = transform.rotate(image, 270)\n","        captions = self.captions_dict[img_name]\n","\n","        if self.img_transform:\n","            image = self.img_transform(image)\n","            image_90 = self.img_transform(image_90)\n","            image_180 = self.img_transform(image_180)\n","            image_270 = self.img_transform(image_270)\n","\n","        if self.captions_transform:\n","            captions, length = self.captions_transform(captions)\n","\n","        sample = {'image': image, 'image_90': image_90, 'image_180': image_180, 'image_270': image_270, 'captions': captions, 'lengths': length}\n","\n","        return sample"]},{"cell_type":"markdown","metadata":{"id":"W0pi-EQYM5eF"},"source":["### Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eF7Q4Q3_9gvQ"},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        # CNN architecture\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=7, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            \n","            # batch 1\n","            nn.Conv2d(64, 64, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(64, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(256, 64, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(64, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(256, 64, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(64, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            # batch 2\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 512, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(512, 128, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 512, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(512, 128, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 512, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(512, 128, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(128, 512, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            # batch 3\n","            nn.Conv2d(512, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            \n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(1024, 256, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(256, 1024, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            # batch 4\n","            nn.Conv2d(1024, 512, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(512, 2048, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(2048, 512, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(512, 2048, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(2048, 512, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(512, 2048, kernel_size=1),\n","            nn.ReLU(inplace = True),\n","        )\n","\n","    def forward(self, image_batch):\n","        # Forward Propogation\n","        encoded_output = self.cnn(image_batch)\n","        encoded_output = encoded_output.permute(0, 2, 3, 1) \n","        return encoded_output\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":620,"status":"ok","timestamp":1636607701802,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"hqy0VaPmxuXe"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, embed_size, train_CNN=False):\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size\n","        self.train_CNN = train_CNN\n","        self.model = models.inception_v3(pretrained=True, aux_logits=False)\n","        self.model.fc = nn.Linear(self.model.fc.in_features, embed_size)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=0.5, inplace=True)\n","\n","    def forward(self, image):\n","        features = self.model(image)\n","        for name, param in self.model.named_parameters():\n","            if \"fc.weight\" in name or \"fc.bias\" in name:\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = self.train_CNN\n","        return self.dropout(self.relu(features))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":824,"status":"ok","timestamp":1636607705998,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"t8UrhbPwMGZU"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n","        super(Decoder, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.dropout = nn.Dropout(p=0.5, inplace=True)\n","        \n","    def forward(self, features, captions, lengths):\n","        embeddings = self.embed(captions)\n","        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # Concatenate image enocded features with embedded captions\n","        packed_seq = torch.nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True)\n","        hiddens, _ = self.lstm(packed_seq)\n","        outputs = self.dropout(self.linear(hiddens[0]))  \n","        return outputs\n"]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":922,"status":"ok","timestamp":1636608231737,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"6eaz6MgvM5eG"},"outputs":[],"source":["class ImageCaptionsNet(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, train_CNN=False):\n","        super(ImageCaptionsNet, self).__init__()\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","        self.num_layers = num_layers\n","        self.encoder = Encoder(embed_size, train_CNN)\n","        self.decoder = Decoder(embed_size, hidden_size, vocab_size, num_layers)\n","\n","\n","    def forward(self, image, captions, lengths):\n","        features = self.encoder(image)\n","        lengths, seq = lengths.sort(descending=True)\n","        image = image[seq]\n","        captions = captions[seq]\n","        outputs = self.decoder(features, captions, lengths)\n","        return outputs\n","    \n","    def caption_image_greedy(self, image, vocabulary, max_length=50):\n","        result_caption = []\n","        with torch.no_grad():\n","            x = self.encoder(image).unsqueeze(1)\n","            states = None\n","            for _ in range(max_length):\n","                hiddens, states = self.decoder.lstm(x, states)\n","                outputs = self.decoder.linear(hiddens.squeeze(1))\n","                predicted = outputs.argmax(1)\n","                result_caption.append(predicted.item())\n","                x = self.decoder.embed(predicted).unsqueeze(1)\n","                if vocabulary[predicted.item()] == '[END]':\n","                    break\n","        return [vocabulary[idx] for idx in result_caption]\n","    \n","    def caption_image_beam_search(self, image, vocabulary, beam_size=10, max_length=50):\n","        with torch.no_grad():\n","            x = self.encoder(image).unsqueeze(1)\n","            states = None\n","            hiddens, states = self.decoder.lstm(x, states)\n","            outputs = self.decoder.linear(hiddens.squeeze(1))\n","            prob_outputs = F.log_softmax(outputs[0], dim=0)\n","            values, idx = torch.topk(prob_outputs, beam_size)\n","            prev_beam = []\n","            next_beam = []\n","            resulting_captions = []\n","            # initialise beam\n","            for i in idx:\n","                prev_beam.append(([i], prob_outputs[i], states))\n","            for _ in range(max_length):\n","                for word_list, prob, hidden_state in prev_beam:\n","                    last_word = self.decoder.embed(word_list[-1]).unsqueeze(0).unsqueeze(0)\n","                    outs, hidden_state = self.decoder.lstm(last_word, hidden_state)\n","                    prob_outputs = F.log_softmax(self.decoder.linear(outs.squeeze(1))[0], dim=0)\n","                    values, idx = torch.topk(prob_outputs, beam_size)\n","                    for i in idx:\n","                        next_beam.append((word_list + [i], prob + prob_outputs[i], hidden_state))\n","                # select top beam_size from beam_size * beam_size entries\n","                next_beam.sort(reverse=True, key=lambda x: x[1])\n","                prev_beam = []\n","                counter = 0\n","                for word_list, prob, hidden_state in next_beam:\n","                    \n","                    if vocabulary[word_list[-1].item()] == '[END]':\n","                        resulting_captions.append((word_list, prob))\n","                    else:\n","                        prev_beam.append((word_list, prob, hidden_state))\n","                        counter += 1\n","                    if counter == beam_size:\n","                        break\n","                next_beam = []\n","                if prev_beam == []:\n","                    break\n","            resulting_captions.sort(reverse=True, key=lambda x: x[1])\n","            caption = resulting_captions[0][0] if resulting_captions != [] else []\n","            if caption == []:\n","                return ['[START]', '[END]']\n","            else:\n","                return [vocabulary[idx.item()] for idx in caption]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmqfKZIPM5eI"},"outputs":[],"source":["IMAGE_DIR = 'train_data'\n","\n","# Creating the Dataset\n","train_dataset = ImageCaptionsDataset(\n","    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n","    captions_transform=captions_preprocessing_obj.captions_transform\n",")\n","\n","# Define your hyperparameters\n","NUMBER_OF_EPOCHS = 1\n","LEARNING_RATE = 1e-3\n","BATCH_SIZE = 8\n","NUM_WORKERS = 2 # Parallel threads for dataloading\n","\n","# define model parameters\n","embed_size = 512\n","hidden_size = 512\n","vocab_size = len(captions_preprocessing_obj.vocab.keys())\n","num_layers = 1\n","\n","net = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers, False)\n","net = net.cuda()\n","\n","loss_function = nn.CrossEntropyLoss()\n","# optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True)\n","optimizer = optim.Adam(net.parameters())\n","\n","\n","# Creating the DataLoader for batching purposes\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","net.train()\n","step = 0\n","for epoch in range(NUMBER_OF_EPOCHS):\n","    for batch_idx, sample in enumerate(train_loader):\n","        # clear gradients\n","        step += 1\n","        image_batch, image_90_batch, image_180_batch, image_270_batch = sample['image'], sample['image_90'], sample['image_180'], sample['image_270']\n","        captions_batch, lengths_batch = sample['captions'], sample['lengths']\n","        image_batch, image_90_batch, image_180_batch, image_270_batch = image_batch.float(), image_90_batch.float(), image_180_batch.float(), image_270_batch.float()\n","        image_batch, image_90_batch, image_180_batch, image_270_batch, captions_batch = image_batch.cuda(), image_90_batch.cuda(), image_180_batch.cuda(), image_270_batch.cuda(), captions_batch.cuda()\n","        lengths_batch, sequence = lengths_batch.sort(descending=True)\n","        captions_batch = captions_batch[sequence]\n","        image_batch, image_90_batch, image_180_batch, image_270_batch = image_batch[sequence], image_90_batch[sequence], image_180_batch[sequence], image_270_batch[sequence]\n","        target_labels = torch.nn.utils.rnn.pack_padded_sequence(captions_batch, lengths_batch, batch_first=True)[0]\n","        \n","        output_captions = net(image_batch, captions_batch, lengths_batch)\n","        loss = loss_function(output_captions, target_labels)\n","        net.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        output_captions = net(image_90_batch, captions_batch, lengths_batch)\n","        loss = loss_function(output_captions, target_labels)\n","        net.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        output_captions = net(image_180_batch, captions_batch, lengths_batch)\n","        loss = loss_function(output_captions, target_labels)\n","        net.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        output_captions = net(image_270_batch, captions_batch, lengths_batch)\n","        loss = loss_function(output_captions, target_labels)\n","        net.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if step % 10 == 0:\n","            print(\"Iteration: \" + str(step) + \", Loss: \" + str(loss.item()))\n","    print(\"Iteration: \" + str(epoch + 1))\n","torch.save(net.state_dict(), \"inception.pth\")"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"jiHy4zzc_bq4"},"outputs":[{"data":{"text/plain":["ImageCaptionsNet(\n","  (encoder): Encoder(\n","    (model): Inception3(\n","      (Conv2d_1a_3x3): BasicConv2d(\n","        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (Conv2d_2a_3x3): BasicConv2d(\n","        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (Conv2d_2b_3x3): BasicConv2d(\n","        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (Conv2d_3b_1x1): BasicConv2d(\n","        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (Conv2d_4a_3x3): BasicConv2d(\n","        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (Mixed_5b): InceptionA(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch5x5_1): BasicConv2d(\n","          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch5x5_2): BasicConv2d(\n","          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_1): BasicConv2d(\n","          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_2): BasicConv2d(\n","          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3): BasicConv2d(\n","          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_5c): InceptionA(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch5x5_1): BasicConv2d(\n","          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch5x5_2): BasicConv2d(\n","          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_1): BasicConv2d(\n","          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_2): BasicConv2d(\n","          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3): BasicConv2d(\n","          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_5d): InceptionA(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch5x5_1): BasicConv2d(\n","          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch5x5_2): BasicConv2d(\n","          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_1): BasicConv2d(\n","          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_2): BasicConv2d(\n","          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3): BasicConv2d(\n","          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_6a): InceptionB(\n","        (branch3x3): BasicConv2d(\n","          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_1): BasicConv2d(\n","          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_2): BasicConv2d(\n","          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3): BasicConv2d(\n","          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_6b): InceptionC(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_1): BasicConv2d(\n","          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_2): BasicConv2d(\n","          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_3): BasicConv2d(\n","          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_1): BasicConv2d(\n","          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_2): BasicConv2d(\n","          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_3): BasicConv2d(\n","          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_4): BasicConv2d(\n","          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_5): BasicConv2d(\n","          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_6c): InceptionC(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_1): BasicConv2d(\n","          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_2): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_3): BasicConv2d(\n","          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_1): BasicConv2d(\n","          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_2): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_3): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_4): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_5): BasicConv2d(\n","          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_6d): InceptionC(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_1): BasicConv2d(\n","          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_2): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_3): BasicConv2d(\n","          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_1): BasicConv2d(\n","          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_2): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_3): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_4): BasicConv2d(\n","          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_5): BasicConv2d(\n","          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_6e): InceptionC(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_2): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7_3): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_2): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_3): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_4): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7dbl_5): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (AuxLogits): None\n","      (Mixed_7a): InceptionD(\n","        (branch3x3_1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3_2): BasicConv2d(\n","          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7x3_1): BasicConv2d(\n","          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7x3_2): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7x3_3): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch7x7x3_4): BasicConv2d(\n","          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_7b): InceptionE(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3_1): BasicConv2d(\n","          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3_2a): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3_2b): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_1): BasicConv2d(\n","          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_2): BasicConv2d(\n","          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3a): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3b): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (Mixed_7c): InceptionE(\n","        (branch1x1): BasicConv2d(\n","          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3_1): BasicConv2d(\n","          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3_2a): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3_2b): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_1): BasicConv2d(\n","          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_2): BasicConv2d(\n","          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3a): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch3x3dbl_3b): BasicConv2d(\n","          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (branch_pool): BasicConv2d(\n","          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (fc): Linear(in_features=2048, out_features=512, bias=True)\n","    )\n","    (relu): ReLU()\n","    (dropout): Dropout(p=0.5, inplace=True)\n","  )\n","  (decoder): Decoder(\n","    (embed): Embedding(7738, 512)\n","    (lstm): LSTM(512, 512, batch_first=True)\n","    (linear): Linear(in_features=512, out_features=7738, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=True)\n","  )\n",")"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["IMAGE_DIR = 'train_data'\n","\n","# Creating the Dataset\n","train_dataset = ImageCaptionsDataset(\n","    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n","    captions_transform=captions_preprocessing_obj.captions_transform\n",")\n","\n","# Define your hyperparameters\n","NUMBER_OF_EPOCHS = 1\n","LEARNING_RATE = 1e-3\n","BATCH_SIZE = 8\n","NUM_WORKERS = 2 # Parallel threads for dataloading\n","\n","# define model parameters\n","embed_size = 512\n","hidden_size = 512\n","vocab_size = len(captions_preprocessing_obj.vocab.keys())\n","num_layers = 1\n","\n","\n","# Creating the DataLoader for batching purposes\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","\n","net = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers, False)\n","net.load_state_dict(torch.load(\"inception.pth\", map_location=torch.device('cpu')))\n","net.eval()"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":736,"status":"ok","timestamp":1636608256025,"user":{"displayName":"Arnav Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15374654642209720139"},"user_tz":-330},"id":"idtCnbZb__JE"},"outputs":[],"source":["idx2word = {}\n","for key, value in captions_preprocessing_obj.vocab.items():\n","    idx2word[value] = key"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wb9-1cLZAyrh","outputId":"92a991b7-f74a-4c72-c72b-5eb72b0edaed"},"outputs":[{"name":"stdout","output_type":"stream","text":["['[START]', 'a', 'man', 'in', 'a', 'red', 'jacket', '[END]']\n"]}],"source":["# iter = 0\n","# for idx, sample in enumerate(train_loader):\n","#     iter += 1\n","#     image = sample['image'][0]\n","#     image = image.float()\n","#     image = image.unsqueeze(0)\n","#     caption = sample['captions'][0].tolist()\n","#     print([idx2word[idx] for idx in caption])\n","#     features = net.encoder(image)\n","#     l = net.caption_image_greedy(image, idx2word)\n","#     print(l)\n","#     if iter == 10:\n","#         break\n","\n","image = io.imread('train_data/res69.jpg')\n","image = img_transform(image).float()\n","image = image.unsqueeze(0)\n","features = net.encoder(image)\n","features = features.squeeze(0)\n","l = net.caption_image_beam_search(image, idx2word, 1)\n","print(l)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOaZEYJHqghY"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hehMrug3qiup"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ML Assignment 4.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
